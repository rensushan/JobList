# 梯度量化方法

## 量化对象

主要有以下三个，实际中可能是量化其中的多个甚至全部。

- weight（权重）：weight的量化是最常规也是最常见的。量化weight可达到减少模型大小和memory footprint等目的。

- activation（激活函数输出）：实际中activation往往是占内存使用的大头，因此量化activation不仅可以大大减少memory footprint。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。

- gradient（梯度）：相对上面两者略微小众一些，因为主要用于训练。它主要作用是在分布式计算中减少通信开销，单机训练时也可减少backward时的开销。

## 主流方法

- [Straight-through Estimator (STE)](https://arxiv.org/abs/1308.3432): 这是一种流行的梯度量化方法，它使用直通估计器来逼近损失函数相对于量化权重的梯度。STE 实现简单，可用于多种激活函数。
  
- [Quantization-Aware Training (QAT)](https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf): 这种方法采用量化感知训练算法，考虑了梯度量化过程中引入的量化噪声。与 STE 相比，QAT 可以提供更好的精度，特别是对于浮点模型。

- [Element-wise Gradient Scaling (EWGS)](https://arxiv.org/abs/2104.00903): 这种方法分别量化每个权重的梯度，而不是量化整个模型的梯度。与 STE 和 QAT 相比，WGQ 可以提供更好的精度，尤其是对于参数数量较多的模型。
  
- [Adaptive Gradient Quantization (AGQ)](https://arxiv.org/abs/2010.12460):  这种方法根据梯度的大小自适应地调整量化步长，有助于减少量化误差。与 STE 和 QAT 相比，AGQ 可以提供更好的精度，尤其是对于参数数量较多的模型。

- [Gradient Quantization with Huffman Coding (GQH)](https://arxiv.org/abs/1510.00149): 这种方法使用哈夫曼编码对模型的梯度进行量化。哈夫曼编码是一种可变长度的前缀编码技术，与传统的量化方法相比，它能提供更有效的压缩。

- [Gradient Quantization with Error Feedback (GQEF)](https://arxiv.org/abs/2004.14180):  这种方法利用误差反馈来提高量化模型的准确性。误差反馈用于调整量化步长和量化过程的舍入策略。

- [Quantization-Aware Regularization with Gradient Penalty (QARGP)](https://arxiv.org/pdf/2002.07520.pdf): 这种方法使用正则化项，对偏离预期量化误差的模型进行惩罚。正则化项基于模型损失函数相对于量化权重的梯度。

## FQT论文中梯度量化方法

### 2019年

商汤和北航 [Towards unified int8 training for convolutional neural network](http://arxiv.org/abs/1912.12607)

- **原理** 

   作者发现梯度的分布不遵从一个分布，即不能像权重一样归于高斯分布

- **方法**

   通过收敛性分析方程，发现了可以通过降低学习率和减少梯度量化误差。另外，Unified INT8对梯度误差分析是layer-wise的
   
   Direction Sensitive Gradient Clipping（方向敏感的梯度裁剪）用余弦距离来度量量化前后梯度的偏差，以减少梯度的方向偏差
   
   偏差反向学习率缩放，以避免沿错误方向的非法渐变更新

### 2020年

加州大学 [A Statistical Framework for Low-bitwidth Training of Deep Neural Networks](http://arxiv.org/abs/2010.14298)

- **原理**

   利用梯度矩阵的稀疏性

- **方法**

   Block Householder Quantizer (BHQ)量化梯度，梯度中只有少量的行是重要的，将行分为多个group，再应用Householder变换得到一个Householder矩阵和一个对角矩阵，降低运算复杂度

### 2021年

阿里巴巴实验室 [Distribution Adaptive INT8 Quantization for Training CNNs](http://arxiv.org/abs/2102.04782)

- **原理** 

   考虑**梯度内部的多重分布**，作者发现在一层中存在着不止一个梯度分布。不同的分布通常具有不同的最优量化参数。实验表明，多个量化参数比一个量化参数能更好地捕捉梯度分布

   考虑**梯度幅值的贡献**，大梯度比小梯度更重要，因为大梯度包含更多的信息。同样，从训练精度的角度考虑，大梯度的量化误差比小梯度的量化误差更重要。随着训练的发展，小梯度占比越来越大，这意味着如果忽略梯度的大小，量化误差和量化参数将由小梯度决定。在这种情况下，大梯度的量化误差将更大，导致最终精度的恶化

- **方法**

   采用**Gradient Vectorized Quantization（梯度矢量化）**对梯度进行量化，作者认为梯度可以channel-wise看（观察每一层梯度），通道维度将梯度的分布分成两种：一个是高斯分布，一个是倒T形分布，然后就对这两种分布采用不同的量化参数。对每一个通道维度使用一个量化参数，C个通道就会有C个量化参数
   
   引入了**Magnitude-aware Clipping Strategy（幅度感知裁剪策略）**，根据观察到的两种梯度分布，分别求解量化参数，本质是在寻找最优截断阈值s

### 2023年

清华朱军团队 [Training Transformers with 4-bit Integers](https://arxiv.org/abs/2306.11987)

- **原理**

   利用梯度矩阵的稀疏性，将接近0的值（对模型结果不大）不进行计算，从而减少计算时间的消耗

- **方法**

   为了**提高精度**，用BS（位分割）将全精度矩阵拆分成高4bit和低4bit，做两次量化。但这么做会将**计算量翻倍**

   为了将**计算量减半**，用LSS（杠杆分数抽样）引入一个掩码mi对梯度和激活进行采样，生成矩阵M，把M分成左上和右下两块分别乘入高4bit和低4bit中进行INT4计算
